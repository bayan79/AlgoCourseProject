Выявление скрытой рекламы

Работу выполняли: магистры группы АСУ2-20-1м Баяндин Кирилл, Кроха Елизавета, Красилов Георгий 

    [x] Создать файлик README.md с пустым проектом
    [x] Найти примеры скрытой рекламы (чем больше тем лучше, но не суть)
    [x] Придумать алгоритм/схемку/любую логическую предпосылку для написания программы
    [x] Проверить различные параметры модели, и выбрать те, при которых модель работает более точно
    [x] Обучить модель(-и) на основе источников
    [x] Сделать выводы
    
    Данная программа пытается отличать рекламный текст от нерекламного на основе Word2Vec и MPL. 
    
    Для обучения есть два датасета: набор твитов на русском языке и набор рекламных объявлений. (ad_detector.ipynb)
    Все сообщения предобрабатываются, чтобы они максимально просто и понятно выглядели, удаляются знаки, слова приводятся к простой форме, обрабатываются стоп-слова (In[7] files).
    Word2Vec может сказать, какие слова похожи, а какие нет. Он выделяет несколько популярных слов и может описать остальные слова через их вектор похожестей.
                        (Основные параметры:
                        size=1000, Размерность векторов слов
                        window=10, Максимальное расстояние между текущим и предсказанным словом в предложении
                        min_count=10, Игнорирует все слова с общей частотой ниже этой
                        workers=4 Используйте это множество рабочих потоков для обучения модели (более быстрое обучение на многоядерных машинах)
                        )
                        
    Каждое сообщение разбивается на слова, находятся их вектора, складываются -> получается "вектор всего сообщения".
    Рекламные тексты (1) и нерекламные тексты (0) обрабатываются by MLPClassifier. Персептончик обучается не сразу всем датасетом, а batch'ами, пачками по 3000.
                        (Основные параметры:
                        alpha - параметр для периода регуляризации, увеличение альфа может исправить высокую дисперсию, уменьшение альфа может исправить высокое смещение и                                         привести к более сложной границе принятия решения.
                        max_iter - максимальное количество итераций, количество раз использования каждой точки данных.
                        )
                        
    Обученному классификатору дается 3 превращенных в вектор тест-текста (один рекламный и два нерекламных) 
    predict() возвращает набор 0 и 1, где 0 означает нерекламный текст, 1 - рекламный. В примере выводится [1, 0, 0], что соответсвует истине.
                        (Не все тесты срабатывают, поэтому необходима доработка, в том числе эксперименты с параметрами методов Word2Vec и MPL)
    
    Кроме того, написан генератор рандомного(но почти осознанного) текста на основе данного текстового файла. Так, в дальнейшем, можно будет генерировать сообщения для обучения,
    как рекламные, так и нет, или использовать в качестве тестов для созданной программы. (generator.py)
